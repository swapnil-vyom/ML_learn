{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***This notebook is developed by Prof. Monali Mavani***"
      ],
      "metadata": {
        "id": "md5aI3u3NIHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstates transfer learning using VGG16 model on image dataset - CIFAR10\n",
        "\n",
        "Dataset: https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html\n",
        "\n",
        "Model: https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html\n",
        "\n",
        "Reference : https://pytorch.org/"
      ],
      "metadata": {
        "id": "M9YqlezRGFMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a pretrained model and modify it for transfer learning.\n",
        "    \n",
        "    \n",
        "    1. Loading a pretrained model (trained on ImageNet)\n",
        "    2. Freezing the feature extraction layers (backbone)\n",
        "    3. Replacing the final classifier for the target dataset"
      ],
      "metadata": {
        "id": "xibbjTS4iby2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fbnQNziwAPd"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "from torchvision import transforms, datasets, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz9uAliqm83a"
      },
      "outputs": [],
      "source": [
        "# Step 1: Initialize Parameters and Hyperparameters\n",
        "# Hyperparameters\n",
        "input_size = 32 * 32 * 3  # CIFAR10 images are 32 * 32 *3\n",
        "\n",
        "num_classes = 10      # CIFAR10 has 10 classes\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K03FkjHUCmkg"
      },
      "source": [
        "**Transforms** are common image transformations. They can be chained together using Compose\n",
        "\n",
        "All transformations accept PIL Image, Tensor Image or batch of Tensor Images as input. Tensor Image is a tensor with (C, H, W) shape, where C is a number of channels, H and W are image height and width. Batch of Tensor Images is a tensor of (B, C, H, W) shape, where B is a number of images in the batch.\n",
        "\n",
        "**transforms.toTensor()** Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "\n",
        "**transforms.Normalize((0.5,), (0.5,)** :  \n",
        "normalizes the data of [0,1] into [-1,1] scale by performing (input-0.5)/0.5, so [0,1]->[-1,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gemuQQNdC-PR"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3KGt871Jigd",
        "outputId": "d077fa92-62bc-455d-f8e6-0fb4065bbd4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltkYfkGYJnsV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9JUQzBJOEY_"
      },
      "source": [
        "### Understanding torchvision datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QpMNLYwLQN7",
        "outputId": "6774a091-19f0-4fc0-a9c5-bbc4fb722d7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# CIFAR 10 dataset\n",
        "\n",
        "train_dataset1 = torchvision.datasets.CIFAR10(root='./data', train=True, transform=None, download=True)\n",
        "test_dataset1 = torchvision.datasets.CIFAR10(root='./data', train=False, transform=None)\n",
        "\n",
        "len(train_dataset1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCTbSpdiLhEa",
        "outputId": "00676826-1c14-478e-ffc2-a1888e6c0a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(test_dataset1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBbON3xHLmoU",
        "outputId": "cf2ca7da-204d-41b8-ebc3-d4944d3a76bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=32x32>, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#first element is a PIL image and the second is an target\n",
        "train_dataset1[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOA_nIwyL-7s",
        "outputId": "62671a89-5b7f-4e29-8468-035eb36efa3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32)\n"
          ]
        }
      ],
      "source": [
        "train_image_zero, train_target_zero = train_dataset1[0]\n",
        "train_image_zero\n",
        "print(train_image_zero.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nps6g4oN_kZ",
        "outputId": "611b4821-d41c-4d67-ec55-0c9c0d604ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32)\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "print(train_image_zero.size)\n",
        "print(train_target_zero)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cfucTDlOK-w"
      },
      "source": [
        "### loading dataset using data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ7tP9WcDCp3"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load CIFAR dataset and perform image pre processing\n",
        "# MNIST dataset (32x32X3images of digits 0-9)\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEwpQOjeLADZ",
        "outputId": "7d2a08c2-e24d-48db-c593-c19610189422"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-0.5373, -0.6627, -0.6078,  ...,  0.2392,  0.1922,  0.1608],\n",
              "          [-0.8745, -1.0000, -0.8588,  ..., -0.0353, -0.0667, -0.0431],\n",
              "          [-0.8039, -0.8745, -0.6157,  ..., -0.0745, -0.0588, -0.1451],\n",
              "          ...,\n",
              "          [ 0.6314,  0.5765,  0.5529,  ...,  0.2549, -0.5608, -0.5843],\n",
              "          [ 0.4118,  0.3569,  0.4588,  ...,  0.4431, -0.2392, -0.3490],\n",
              "          [ 0.3882,  0.3176,  0.4039,  ...,  0.6941,  0.1843, -0.0353]],\n",
              " \n",
              "         [[-0.5137, -0.6392, -0.6235,  ...,  0.0353, -0.0196, -0.0275],\n",
              "          [-0.8431, -1.0000, -0.9373,  ..., -0.3098, -0.3490, -0.3176],\n",
              "          [-0.8118, -0.9451, -0.7882,  ..., -0.3412, -0.3412, -0.4275],\n",
              "          ...,\n",
              "          [ 0.3333,  0.2000,  0.2627,  ...,  0.0431, -0.7569, -0.7333],\n",
              "          [ 0.0902, -0.0353,  0.1294,  ...,  0.1608, -0.5137, -0.5843],\n",
              "          [ 0.1294,  0.0118,  0.1137,  ...,  0.4431, -0.0745, -0.2784]],\n",
              " \n",
              "         [[-0.5059, -0.6471, -0.6627,  ..., -0.1529, -0.2000, -0.1922],\n",
              "          [-0.8431, -1.0000, -1.0000,  ..., -0.5686, -0.6078, -0.5529],\n",
              "          [-0.8353, -1.0000, -0.9373,  ..., -0.6078, -0.6078, -0.6706],\n",
              "          ...,\n",
              "          [-0.2471, -0.7333, -0.7961,  ..., -0.4510, -0.9451, -0.8431],\n",
              "          [-0.2471, -0.6706, -0.7647,  ..., -0.2627, -0.7333, -0.7333],\n",
              "          [-0.0902, -0.2627, -0.3176,  ...,  0.0980, -0.3412, -0.4353]]]),\n",
              " 6)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F8YwBQktth3"
      },
      "source": [
        "DataLoader()\n",
        "The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
        "\n",
        "DataLoader is an iterable that abstracts this complexity for us in an easy API. PyTorch Dataloader is a utility class designed to simplify loading and iterating over datasets while training deep learning models. It has various constraints to iterating datasets, like batching, shuffling, and processing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL9gWD0bO4uh"
      },
      "outputs": [],
      "source": [
        "# DataLoader returns the batched data (input features and labels) to the training loop.\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoPaq_tTTJ-S",
        "outputId": "a6447eb5-e76f-4a62-cf2a-c668532f827b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 32, 32]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "#iterating over dataloader, each batch size is 10000\n",
        "i=0\n",
        "for X_batch, y_batch in train_loader:\n",
        "    #print(X_batch, y_batch)\n",
        "    print(X_batch.shape, y_batch.shape)\n",
        "    i=i+1\n",
        "    if i==1:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia9usP29UoVp"
      },
      "source": [
        "### Define the Neural Network using torch.nn.Module\n",
        "Base class for all neural network modules. Your models should also subclass this class.\n",
        "\n",
        "An nn.Module contains layers, and a method forward(input) that returns the output.\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjCk4La-KCfG"
      },
      "outputs": [],
      "source": [
        "def get_pretrained_model(model_name):\n",
        "\n",
        "    if model_name == 'vgg16':\n",
        "        model = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Freeze early layers\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "# === CLASSIFIER MODIFICATION ===\n",
        "        # Get the number of input features to the final classifier layer\n",
        "        # VGG16 architecture: features -> avgpool -> classifier\n",
        "        # classifier[6] is the final FC layer: Linear(4096 -> 1000) for ImageNet\n",
        "\n",
        "        n_inputs = model.classifier[6].in_features\n",
        "        print( \"VGG 16 final conv layer output size\",n_inputs)\n",
        "        # Add on classifier\n",
        "#In VGG16, the final classifier layer (commonly referred to as classifier[6]) is a fully-connected (FC) layer responsible for mapping\n",
        "#features extracted by the network to class predictions, changing the original 1000-class output layer (from ImageNet) into a smaller\n",
        " #(or differently sized) classifier layer for your dataset with num_classes\n",
        "        model.classifier[6] = nn.Sequential(\n",
        "            nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes), nn.LogSoftmax(dim=1))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih-fnIVUHw2u",
        "outputId": "b22f9a9e-2718-4366-9b45-a98a861d9c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:04<00:00, 135MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4096\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [64, 64, 32, 32]           1,792\n",
            "              ReLU-2           [64, 64, 32, 32]               0\n",
            "            Conv2d-3           [64, 64, 32, 32]          36,928\n",
            "              ReLU-4           [64, 64, 32, 32]               0\n",
            "         MaxPool2d-5           [64, 64, 16, 16]               0\n",
            "            Conv2d-6          [64, 128, 16, 16]          73,856\n",
            "              ReLU-7          [64, 128, 16, 16]               0\n",
            "            Conv2d-8          [64, 128, 16, 16]         147,584\n",
            "              ReLU-9          [64, 128, 16, 16]               0\n",
            "        MaxPool2d-10            [64, 128, 8, 8]               0\n",
            "           Conv2d-11            [64, 256, 8, 8]         295,168\n",
            "             ReLU-12            [64, 256, 8, 8]               0\n",
            "           Conv2d-13            [64, 256, 8, 8]         590,080\n",
            "             ReLU-14            [64, 256, 8, 8]               0\n",
            "           Conv2d-15            [64, 256, 8, 8]         590,080\n",
            "             ReLU-16            [64, 256, 8, 8]               0\n",
            "        MaxPool2d-17            [64, 256, 4, 4]               0\n",
            "           Conv2d-18            [64, 512, 4, 4]       1,180,160\n",
            "             ReLU-19            [64, 512, 4, 4]               0\n",
            "           Conv2d-20            [64, 512, 4, 4]       2,359,808\n",
            "             ReLU-21            [64, 512, 4, 4]               0\n",
            "           Conv2d-22            [64, 512, 4, 4]       2,359,808\n",
            "             ReLU-23            [64, 512, 4, 4]               0\n",
            "        MaxPool2d-24            [64, 512, 2, 2]               0\n",
            "           Conv2d-25            [64, 512, 2, 2]       2,359,808\n",
            "             ReLU-26            [64, 512, 2, 2]               0\n",
            "           Conv2d-27            [64, 512, 2, 2]       2,359,808\n",
            "             ReLU-28            [64, 512, 2, 2]               0\n",
            "           Conv2d-29            [64, 512, 2, 2]       2,359,808\n",
            "             ReLU-30            [64, 512, 2, 2]               0\n",
            "        MaxPool2d-31            [64, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-32            [64, 512, 7, 7]               0\n",
            "           Linear-33                 [64, 4096]     102,764,544\n",
            "             ReLU-34                 [64, 4096]               0\n",
            "          Dropout-35                 [64, 4096]               0\n",
            "           Linear-36                 [64, 4096]      16,781,312\n",
            "             ReLU-37                 [64, 4096]               0\n",
            "          Dropout-38                 [64, 4096]               0\n",
            "           Linear-39                  [64, 256]       1,048,832\n",
            "             ReLU-40                  [64, 256]               0\n",
            "          Dropout-41                  [64, 256]               0\n",
            "           Linear-42                   [64, 10]           2,570\n",
            "       LogSoftmax-43                   [64, 10]               0\n",
            "================================================================\n",
            "Total params: 135,311,946\n",
            "Trainable params: 1,051,402\n",
            "Non-trainable params: 134,260,544\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 309.88\n",
            "Params size (MB): 516.17\n",
            "Estimated Total Size (MB): 826.81\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "model = get_pretrained_model('vgg16')\n",
        "model.to(device)\n",
        "summary(model, input_size=(3, 32, 32), batch_size=batch_size, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMImk3ooIzWe",
        "outputId": "81a74db6-689a-468f-90d7-3ae38b37e2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4096, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            "  (3): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (4): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model.classifier[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK85FRquJHVe",
        "outputId": "d85c713a-5f98-4408-c998-2f3bc713128b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'airplane'),\n",
              " (1, 'automobile'),\n",
              " (2, 'bird'),\n",
              " (3, 'cat'),\n",
              " (4, 'deer'),\n",
              " (5, 'dog'),\n",
              " (6, 'frog'),\n",
              " (7, 'horse'),\n",
              " (8, 'ship'),\n",
              " (9, 'truck')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.class_to_idx = train_dataset.class_to_idx\n",
        "model.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in model.class_to_idx.items()\n",
        "}\n",
        "\n",
        "list(model.idx_to_class.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9pL6L6dKlAB"
      },
      "outputs": [],
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "    model = model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqeUi-Z-TgsZ",
        "outputId": "84d6e33c-77e5-4c5a-dc91-36fc81a1a7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable model parameters: 1051402\n"
          ]
        }
      ],
      "source": [
        "#Returns the total number of elements in the input tensor.\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of trainable model parameters:\", num_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk1UBPXdDKlA"
      },
      "outputs": [],
      "source": [
        "# Step 4: Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owdh9Ss5DNSg",
        "outputId": "1a7825a5-13aa-4274-fb07-155d6d3b55b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "batch 100, Loss: 1.485339810848236\n",
            "batch 200, Loss: 1.3010353279113769\n",
            "batch 300, Loss: 1.275811059474945\n",
            "batch 400, Loss: 1.282517575621605\n",
            "batch 500, Loss: 1.2473729878664017\n",
            "batch 600, Loss: 1.2227535676956176\n",
            "batch 700, Loss: 1.2327392476797103\n",
            "Starting epoch 2\n",
            "batch 100, Loss: 1.1877826249599457\n",
            "batch 200, Loss: 1.1785250842571258\n",
            "batch 300, Loss: 1.161914011836052\n",
            "batch 400, Loss: 1.189522801041603\n",
            "batch 500, Loss: 1.1978879243135452\n",
            "batch 600, Loss: 1.1779108756780625\n",
            "batch 700, Loss: 1.180791630744934\n",
            "Starting epoch 3\n",
            "batch 100, Loss: 1.133305692076683\n",
            "batch 200, Loss: 1.1508476078510284\n",
            "batch 300, Loss: 1.146054611802101\n",
            "batch 400, Loss: 1.1484496718645096\n",
            "batch 500, Loss: 1.1763249325752259\n",
            "batch 600, Loss: 1.14164002597332\n",
            "batch 700, Loss: 1.134975568652153\n",
            "Starting epoch 4\n",
            "batch 100, Loss: 1.1022539538145066\n",
            "batch 200, Loss: 1.1043799358606339\n",
            "batch 300, Loss: 1.106764291524887\n",
            "batch 400, Loss: 1.1287304383516312\n",
            "batch 500, Loss: 1.1265617042779923\n",
            "batch 600, Loss: 1.1309332156181335\n",
            "batch 700, Loss: 1.1438132548332214\n",
            "Starting epoch 5\n",
            "batch 100, Loss: 1.0900414383411408\n",
            "batch 200, Loss: 1.076435078382492\n",
            "batch 300, Loss: 1.0987681484222411\n",
            "batch 400, Loss: 1.117142654657364\n",
            "batch 500, Loss: 1.1183044022321702\n",
            "batch 600, Loss: 1.1043531209230424\n",
            "batch 700, Loss: 1.113718485236168\n",
            "Training process has been completed. \n",
            "Training time: 0:01:54.107933\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Train the model\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Starting epoch {epoch + 1}')\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train() # Set the model to training mode\n",
        "\n",
        "    for i, batch in enumerate(train_loader,0):\n",
        "\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device) # Move the model to the device\n",
        "\n",
        "        # Forward propagate\n",
        "        outputs = model(images) # Forward pass\n",
        "        loss = criterion(outputs, labels) # Compute the loss\n",
        "\n",
        "        # Backpropagate\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        loss.backward() # Backward pass (compute gradients)\n",
        "\n",
        "       # Update parameters\n",
        "        optimizer.step() # Update model parameters\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "             print(f'batch {i+1}, Loss: {running_loss/100}')\n",
        "             running_loss = 0.0\n",
        "\n",
        "   # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "end_time = time.time() # Record end time\n",
        "print('Training process has been completed. ')\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print('Training time:', str(datetime.timedelta(seconds=training_time))) # for calculating the training time in minutes and seconds format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns7YxLVPDQjw",
        "outputId": "13645d2b-1f62-4405-a2af-bf9a2856f0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 62.12%\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Evaluate model performance\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in test_loader:\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1) #Get the predicted classes from the output logits\n",
        "\n",
        "        total += labels.size(0) #labels:Tensor with dimensions [no of samples in the batch, 1], labels.size(0) returns no of samples in a batch\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "MyYUa87eo8sm",
        "outputId": "f1a6d58e-e672-40d1-833c-ce2f36f08ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label for the first test image: 1\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 7: Use the model for inference\n",
        "sample_image, _ = test_dataset[6]  # Take the sixth test sample\n",
        "\n",
        "# Display the test image\n",
        "plt.imshow(sample_image.permute(1, 2, 0))\n",
        "#plt.imshow(sample_image.squeeze(), cmap='gray')  # Remove the batch dimension and display the image\n",
        "plt.title('Test Image')\n",
        "plt.show()\n",
        "\n",
        "# Add batch dimension and move the image to the device\n",
        "sample_image = sample_image.unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Perform inference without tracking gradients\n",
        "with torch.no_grad():\n",
        "    prediction = model(sample_image)\n",
        "    predicted_label = torch.argmax(prediction, 1).item()  # Get the predicted class label\n",
        "    print(f'Predicted label for the first test image: {predicted_label}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}